---
title: Flow of Forward and Back Propagation in Neural Network Training
subtitle:
layout: image
entry-type: image
entry-by: Jonathan Reus

# if a url is available, put it here, otherwise leave empty
source-url: https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795
source: Piotr Skalski, Let's Code a Neural Network in Plain Numpy
# what is the copyright?
copyright:

summary: Process flow of computation during training of a neural-network model. Illustrated is computation of input to output layers as a forward propagation of statistical activations, followed by an error evaluation and back-propagation adjusting statistical weights.

image: algorithms/forward-back-propagation-neuralnetworks.gif

# all tags separated by commas
tags: [training, neural networks]
weights: [0.95, 0.99]
categories: modelling

# permalink: /entry1/

---
